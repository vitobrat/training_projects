{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9a00c0df-3d33-4d12-aa6d-f18c11036b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.9.0.80-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\bratk\\anaconda3\\lib\\site-packages (from opencv-python) (1.26.4)\n",
      "Downloading opencv_python-4.9.0.80-cp37-abi3-win_amd64.whl (38.6 MB)\n",
      "   ---------------------------------------- 0.0/38.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/38.6 MB 1.3 MB/s eta 0:00:31\n",
      "   ---------------------------------------- 0.2/38.6 MB 2.0 MB/s eta 0:00:20\n",
      "   ---------------------------------------- 0.3/38.6 MB 2.6 MB/s eta 0:00:15\n",
      "    --------------------------------------- 0.8/38.6 MB 4.3 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 1.8/38.6 MB 8.4 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 4.1/38.6 MB 15.5 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 5.2/38.6 MB 18.6 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 9.4/38.6 MB 26.2 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 11.7/38.6 MB 50.4 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 13.9/38.6 MB 50.4 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 16.0/38.6 MB 59.5 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 18.2/38.6 MB 50.4 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 20.5/38.6 MB 50.4 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 22.7/38.6 MB 50.1 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 24.7/38.6 MB 46.9 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 27.3/38.6 MB 46.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 29.4/38.6 MB 50.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 31.9/38.6 MB 50.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 34.5/38.6 MB 50.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 37.1/38.6 MB 54.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 37.4/38.6 MB 46.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 37.4/38.6 MB 46.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 37.5/38.6 MB 36.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 37.6/38.6 MB 28.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.2/38.6 MB 25.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.6/38.6 MB 25.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 38.6/38.6 MB 22.6 MB/s eta 0:00:00\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.9.0.80\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "067de2ec-0726-4d00-acea-bcb04d903bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "import torchvision as tv\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision as tv\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6f6996a8-2bcb-4653-beb4-07c981eda88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, num_channel):\n",
    "        super().__init__()\n",
    "        self.conv0 = nn.Conv2d(num_channel, num_channel, kernel_size=3, padding=\"same\")\n",
    "        self.norm0 = nn.BatchNorm2d(num_channel)\n",
    "        self.activation = nn.LeakyReLU(0.2)\n",
    "        self.conv1 = nn.Conv2d(num_channel, num_channel, kernel_size=3, padding=\"same\")\n",
    "        self.norm1 = nn.BatchNorm2d(num_channel)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv0(x)\n",
    "        out = self.norm0(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.conv1(out)\n",
    "        out = self.norm1(out)\n",
    "        out = self.activation(x + out)\n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "668002e2-6b4d-49cb-b9a8-93538fb49746",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleNeckBlock(nn.Module):\n",
    "    def __init__(self, num_channel):\n",
    "        super().__init__()\n",
    "        self.conv0 = nn.Conv2d(num_channel, num_channel // 4, kernel_size=1)\n",
    "        self.norm0 = nn.BatchNorm2d(num_channel // 4)\n",
    "        self.activation = nn.LeakyReLU(0.2)\n",
    "        self.conv1 = nn.Conv2d(num_channel // 4, num_channel // 4, kernel_size=3, padding=\"same\")\n",
    "        self.norm1 = nn.BatchNorm2d(num_channel // 4)\n",
    "        self.conv2 = nn.Conv2d(num_channel // 4, num_channel, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv0(x)\n",
    "        out = self.norm0(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.conv1(out)\n",
    "        out = self.norm1(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.conv2(out)\n",
    "        out = x + out\n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7ca919e1-850a-4c83-bc0d-edea972899cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResTruck(nn.Module):\n",
    "    def __init__(self, num_channel, num_blocks, block_type = \"classic\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        truck = []\n",
    "        for i in range(num_blocks):\n",
    "            if block_type == \"classic\":\n",
    "                truck += [ResBlock(num_channel)]\n",
    "            else:\n",
    "                truck += [BottleNeckBlock(num_channel)]\n",
    "\n",
    "        self.block = nn.Sequential(*truck)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5357a141-a11c-4971-9dbe-1704ee7f33a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, input_num_channel, num_channel, exit_num_channel):\n",
    "        super().__init__()\n",
    "        self.conv0 = nn.Conv2d(input_num_channel, num_channel, kernel_size=5, stride=2)\n",
    "        \n",
    "        self.activation = nn.LeakyReLU(0.2)\n",
    "        self.maxpool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.block1 = ResTruck(num_channel, 2)\n",
    "        self.conv1 = nn.Conv2d(num_channel, 2*num_channel, kernel_size=3, padding=1, stride=2)\n",
    "        self.block2 = ResTruck(2*num_channel, 2, \"nonclassic\")\n",
    "        self.conv2 = nn.Conv2d(2*num_channel, 4*num_channel, kernel_size=3, padding=1, stride=2)\n",
    "        self.block3 = ResTruck(4*num_channel, 3, \"nonclassic\")\n",
    "        self.conv3 = nn.Conv2d(4*num_channel, 4*num_channel, kernel_size=3, padding=1, stride=2)\n",
    "        self.block4 = ResTruck(4*num_channel, 2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(4*num_channel, exit_num_channel)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv0(x)\n",
    "        out = self.activation(out)\n",
    "        out = self.maxpool(out)\n",
    "\n",
    "        out = self.block1(out)\n",
    "        out = self.conv1(out)\n",
    "        out = self.block2(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.block3(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.block4(out)\n",
    "\n",
    "        out = self.avgpool(out)\n",
    "        out = self.flatten(out)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f935fad-dccc-4700-a523-84e959f353e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4759d28f-d218-4939-a58b-6c7f282e4f45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Dataset2class(torch.utils.data.Dataset):\n",
    "    def __init__(self, path_dir1:str):\n",
    "        super().__init__()\n",
    "\n",
    "        self.path_dir1 = path_dir1\n",
    "        self.dir1_list = sorted(os.listdir(path_dir1))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dir1_list)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        if self.dir1_list[key].split(\".\")[0] == \"cat\":\n",
    "            id_class = 1\n",
    "            img_path = os.path.join(self.path_dir1, self.dir1_list[key])\n",
    "        else: \n",
    "            id_class = 0\n",
    "            img_path = os.path.join(self.path_dir1, self.dir1_list[key])\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = img.astype(np.float32)\n",
    "        img = img / 255.0\n",
    "        img = cv2.resize(img, (224, 224), interpolation=cv2.INTER_AREA)\n",
    "        img = img.transpose((2, 0, 1))\n",
    "        t_img = torch.from_numpy(img)\n",
    "        t_class_id = torch.tensor([id_class])\n",
    "\n",
    "        return {\"img\" : t_img,\n",
    "                \"labels\" : t_class_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "005bea36-271b-4ca2-8e07-16d4d5ec2450",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def accuracy(labels, predictions, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Computes accuracy between binary labels and predictions.\n",
    "\n",
    "    Parameters:\n",
    "    - labels (torch.Tensor): Binary ground truth labels (0 or 1).\n",
    "    - predictions (torch.Tensor): Predicted values (e.g., output of a sigmoid activation).\n",
    "    - threshold (float): Threshold for converting predictions to binary values.\n",
    "\n",
    "    Returns:\n",
    "    - float: Accuracy value.\n",
    "    \"\"\"\n",
    "    binary_predictions = (torch.sigmoid(predictions) > threshold).to(torch.float32)\n",
    "    correct_predictions = (binary_predictions == labels).to(torch.float32)\n",
    "    accuracy_value = correct_predictions.mean().item()\n",
    "    return accuracy_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "88c8e2e0-b530-4dd4-acd5-c7208ee1133d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_path = \"C://img//dogs_vs_cats//train\"\n",
    "test_path = \"C://img//Dogs and Cats//dataset//test_set\"\n",
    "train_ds_cats_dogs = Dataset2class(train_path)\n",
    "test_ds_cats_dogs = Dataset2class(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1872d8f1-6b3c-43ad-b859-d3cc767814c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_size = int(0.85 * len(train_ds_cats_dogs))\n",
    "val_size = len(train_ds_cats_dogs) - train_size\n",
    "\n",
    "train_data, val_data = random_split(train_ds_cats_dogs, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False,  num_workers=0, drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_ds_cats_dogs, batch_size=batch_size, num_workers=0, shuffle=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "055b0701-c426-4f17-94f1-079c1c281888",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "716cc113-7cd0-4c4c-ac62-6d3518d1bf81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "444c0cdb-b4aa-472e-89d5-212acd4230b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv0): Conv2d(3, 64, kernel_size=(5, 5), stride=(2, 2))\n",
      "  (activation): LeakyReLU(negative_slope=0.2)\n",
      "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (block1): ResTruck(\n",
      "    (block): Sequential(\n",
      "      (0): ResBlock(\n",
      "        (conv0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): LeakyReLU(negative_slope=0.2)\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): ResBlock(\n",
      "        (conv0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): LeakyReLU(negative_slope=0.2)\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (block2): ResTruck(\n",
      "    (block): Sequential(\n",
      "      (0): BottleNeckBlock(\n",
      "        (conv0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (norm0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): LeakyReLU(negative_slope=0.2)\n",
      "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (1): BottleNeckBlock(\n",
      "        (conv0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (norm0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): LeakyReLU(negative_slope=0.2)\n",
      "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (block3): ResTruck(\n",
      "    (block): Sequential(\n",
      "      (0): BottleNeckBlock(\n",
      "        (conv0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): LeakyReLU(negative_slope=0.2)\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (1): BottleNeckBlock(\n",
      "        (conv0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): LeakyReLU(negative_slope=0.2)\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (2): BottleNeckBlock(\n",
      "        (conv0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): LeakyReLU(negative_slope=0.2)\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (conv3): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (block4): ResTruck(\n",
      "    (block): Sequential(\n",
      "      (0): ResBlock(\n",
      "        (conv0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (norm0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): LeakyReLU(negative_slope=0.2)\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): ResBlock(\n",
      "        (conv0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (norm0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation): LeakyReLU(negative_slope=0.2)\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "3721089\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = ResNet(3, 64, 1)\n",
    "model = model.to(device)\n",
    "print(model)\n",
    "print(count_parameters(model))\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ce7b1506-d4be-4502-a4a2-eb829098d8c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a5a19e43-70f9-48e1-a2e3-2e3ffbbaa58f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "use_amp = True\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "torch.backends.cudnn.benchmark = True #проверяет сначала какой алгоритм вычисления тензоров\n",
    "#эффективный и потом по нему производит все вычисления. Необходим фиксированный размер фотографии\n",
    "torch.backends.cudnn.deterministic = False #работает как seed=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3da4d6de-f96c-411b-932e-5891e2c05c15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss_train: 0.641, accuracy_train: 0.625: 100%|██████████| 332/332 [01:50<00:00,  3.01it/s]\n",
      "100%|██████████| 58/58 [00:18<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train Loss: 0.660, Train Accuracy: 0.608\n",
      "Epoch 1 - Validation Loss: 0.714, Validation Accuracy: 0.551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss_train: 0.640, accuracy_train: 0.688: 100%|██████████| 332/332 [01:51<00:00,  2.97it/s]\n",
      "100%|██████████| 58/58 [00:16<00:00,  3.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Train Loss: 0.571, Train Accuracy: 0.702\n",
      "Epoch 2 - Validation Loss: 0.593, Validation Accuracy: 0.662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss_train: 0.451, accuracy_train: 0.797: 100%|██████████| 332/332 [01:40<00:00,  3.30it/s]\n",
      "100%|██████████| 58/58 [00:16<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Train Loss: 0.504, Train Accuracy: 0.749\n",
      "Epoch 3 - Validation Loss: 0.628, Validation Accuracy: 0.658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss_train: 0.520, accuracy_train: 0.734: 100%|██████████| 332/332 [01:40<00:00,  3.31it/s]\n",
      "100%|██████████| 58/58 [00:16<00:00,  3.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Train Loss: 0.451, Train Accuracy: 0.789\n",
      "Epoch 4 - Validation Loss: 0.470, Validation Accuracy: 0.796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss_train: 0.401, accuracy_train: 0.812: 100%|██████████| 332/332 [01:40<00:00,  3.31it/s]\n",
      "100%|██████████| 58/58 [00:16<00:00,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Train Loss: 0.388, Train Accuracy: 0.827\n",
      "Epoch 5 - Validation Loss: 0.580, Validation Accuracy: 0.736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss_train: 0.386, accuracy_train: 0.859: 100%|██████████| 332/332 [01:39<00:00,  3.34it/s]\n",
      "100%|██████████| 58/58 [00:16<00:00,  3.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Train Loss: 0.317, Train Accuracy: 0.860\n",
      "Epoch 6 - Validation Loss: 0.478, Validation Accuracy: 0.780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss_train: 0.349, accuracy_train: 0.859: 100%|██████████| 332/332 [01:39<00:00,  3.33it/s]\n",
      "100%|██████████| 58/58 [00:16<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Train Loss: 0.270, Train Accuracy: 0.886\n",
      "Epoch 7 - Validation Loss: 0.324, Validation Accuracy: 0.856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss_train: 0.093, accuracy_train: 1.000: 100%|██████████| 332/332 [01:39<00:00,  3.33it/s]\n",
      "100%|██████████| 58/58 [00:16<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Train Loss: 0.236, Train Accuracy: 0.903\n",
      "Epoch 8 - Validation Loss: 0.358, Validation Accuracy: 0.835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss_train: 0.217, accuracy_train: 0.906: 100%|██████████| 332/332 [01:41<00:00,  3.28it/s]\n",
      "100%|██████████| 58/58 [00:17<00:00,  3.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Train Loss: 0.198, Train Accuracy: 0.917\n",
      "Epoch 9 - Validation Loss: 0.241, Validation Accuracy: 0.898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss_train: 0.184, accuracy_train: 0.938: 100%|██████████| 332/332 [01:41<00:00,  3.29it/s]\n",
      "100%|██████████| 58/58 [00:16<00:00,  3.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Train Loss: 0.179, Train Accuracy: 0.927\n",
      "Epoch 10 - Validation Loss: 0.317, Validation Accuracy: 0.860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    loss_val = 0\n",
    "    loss_train = 0\n",
    "    accuracy_val = 0\n",
    "    accuracy_train = 0\n",
    "    \n",
    "    model.train()\n",
    "    for sample in (pbar := tqdm(train_loader)):\n",
    "        train_img = sample[\"img\"].to(device)\n",
    "        train_labels = sample[\"labels\"].float().to(device) \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast(use_amp):\n",
    "            pred = model(train_img)\n",
    "            loss = loss_fn(pred, train_labels)\n",
    "        \n",
    "        if device == \"cuda\" and use_amp:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            loss_item_train = loss.item()\n",
    "            loss_train += loss_item_train\n",
    "\n",
    "            accuracy_current_train = accuracy(train_labels, pred)\n",
    "            accuracy_train += accuracy_current_train\n",
    "        pbar.set_description(f\"loss_train: {loss_item_train:.3f}, accuracy_train: {accuracy_current_train:.3f}\")\n",
    "        \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for sample_val in tqdm(val_loader):\n",
    "            img_val = sample_val[\"img\"].to(device)\n",
    "            label_val = sample_val[\"labels\"].float().to(device) \n",
    "            \n",
    "            with autocast(use_amp):\n",
    "                pred_val = model(img_val)\n",
    "                loss_fn_val = loss_fn(pred_val, label_val)\n",
    "\n",
    "                loss_item_val = loss_fn_val.item()\n",
    "                loss_val += loss_item_val\n",
    "\n",
    "                accuracy_current_val = accuracy(label_val, pred_val)\n",
    "                accuracy_val += accuracy_current_val\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} - Train Loss: {loss_train / len(train_loader):.3f}, Train Accuracy: {accuracy_train / len(train_loader):.3f}\")\n",
    "    print(f\"Epoch {epoch + 1} - Validation Loss: {loss_val / len(val_loader):.3f}, Validation Accuracy: {accuracy_val / len(val_loader):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1ebabf16-a2ab-46a4-adb1-06e2eb250ead",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save({'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss_fn}, \"current_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "011281b5-1941-48e7-9329-e02461920e7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "current_model = ResNet(3, 64, 1)\n",
    "current_model = current_model.to(device)\n",
    "optimizer = torch.optim.Adam(current_model.parameters())\n",
    "\n",
    "checkpoint = torch.load(\"current_model\")\n",
    "current_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "loss = checkpoint['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c72b690f-7802-4ff1-91d6-1f6eda0c1f3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.1779381421705087\n",
      "test_accuracy: 0.9255952380952381\n"
     ]
    }
   ],
   "source": [
    "loss_val = 0\n",
    "accuracy_val = 0\n",
    "current_model.eval()\n",
    "for sample in test_loader:\n",
    "    img, labels = sample[\"img\"].to(device), sample[\"labels\"].float().to(device)\n",
    "    with torch.no_grad():\n",
    "        pred = current_model(img)\n",
    "        loss = loss_fn(pred, labels)\n",
    "\n",
    "    loss_item = loss.item()\n",
    "    loss_val += loss_item\n",
    "    accuracy_current = accuracy(labels, pred)\n",
    "    accuracy_val += accuracy_current\n",
    "print(f\"test_loss: {loss_val/len(test_loader)}\")\n",
    "print(f\"test_accuracy: {accuracy_val/len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b16d3ad0-76b8-41d4-923e-628190b06d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.2917527062818408\n",
      "test_accuracy: 0.884765625\n"
     ]
    }
   ],
   "source": [
    "loss_val = 0\n",
    "accuracy_val = 0\n",
    "current_model.eval()\n",
    "for sample in test_loader:\n",
    "    img, labels = sample[\"img\"].to(device), sample[\"labels\"].float().to(device)\n",
    "    with torch.no_grad():\n",
    "        pred = current_model(img)\n",
    "        loss = loss_fn(pred, labels)\n",
    "\n",
    "    loss_item = loss.item()\n",
    "    loss_val += loss_item\n",
    "    accuracy_current = accuracy(labels, pred)\n",
    "    accuracy_val += accuracy_current\n",
    "print(f\"test_loss: {loss_val/len(test_loader)}\")\n",
    "print(f\"test_accuracy: {accuracy_val/len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "43fab2ae-6d91-4dcf-9488-6179a9897e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_img(path):\n",
    "    img = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = img.astype(np.float32)\n",
    "    img = img / 255.0\n",
    "    img = cv2.resize(img, (256, 256), interpolation=cv2.INTER_AREA)\n",
    "    img = img.transpose((2, 0, 1))\n",
    "    t_img = torch.from_numpy(img)\n",
    "    return t_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fca75283-280a-4920-ae8a-81c1c825bff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0148]])\n"
     ]
    }
   ],
   "source": [
    "current_model.eval()\n",
    "current_model.to(\"cpu\")\n",
    "with torch.no_grad():\n",
    "    print(torch.sigmoid(current_model(check_img(\"C://img//Dogs and Cats//dataset//single_prediction//banana_dog.jpg\").unsqueeze(0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30f08ef-026b-4b84-9b70-1a96c197bd9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
