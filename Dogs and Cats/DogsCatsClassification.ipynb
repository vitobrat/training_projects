{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fada7b58-96c4-481a-8dcb-aa770199763a",
   "metadata": {
    "id": "fada7b58-96c4-481a-8dcb-aa770199763a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "import torchvision as tv\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import yaml\n",
    "\n",
    "import torchvision as tv\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b28f0ec-73a4-46c7-be86-79aa012db510",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_path = \"config.yml\"\n",
    "with open(yaml_path, \"r\") as yaml_file:\n",
    "    options = yaml.safe_load(yaml_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffff1ecd-4532-4709-a7c9-dde5a7a5a5af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 32,\n",
       " 'train_path': 'C://img//dogs_vs_cats//train',\n",
       " 'test_path': 'C://img//Dogs and Cats//dataset//test_set',\n",
       " 'network': {'input_size': 3,\n",
       "  'output_size': 1,\n",
       "  'loss': 'nn.BCEWithLogitsLoss()',\n",
       "  'benchmark': True,\n",
       "  'use_amp': False}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd7d4843-ef3d-40bc-936d-f62a85eee722",
   "metadata": {
    "id": "bd7d4843-ef3d-40bc-936d-f62a85eee722",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Dataset2class(torch.utils.data.Dataset):\n",
    "    def __init__(self, path_dir1:str):\n",
    "        super().__init__()\n",
    "\n",
    "        self.path_dir1 = path_dir1\n",
    "        self.dir1_list = sorted(os.listdir(path_dir1))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dir1_list)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        if self.dir1_list[key].split(\".\")[0] == \"cat\":\n",
    "            id_class = 1\n",
    "            img_path = os.path.join(self.path_dir1, self.dir1_list[key])\n",
    "        else: \n",
    "            id_class = 0\n",
    "            img_path = os.path.join(self.path_dir1, self.dir1_list[key])\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = img.astype(np.float32)\n",
    "        img = img / 255.0\n",
    "        img = cv2.resize(img, (224, 224), interpolation=cv2.INTER_AREA)\n",
    "        img = img.transpose((2, 0, 1))\n",
    "        t_img = torch.from_numpy(img)\n",
    "        t_class_id = torch.tensor([id_class])\n",
    "\n",
    "        return {\"img\" : t_img,\n",
    "                \"labels\" : t_class_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6147534f-6742-44d8-b639-1c4213750faa",
   "metadata": {
    "id": "6147534f-6742-44d8-b639-1c4213750faa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.activation = nn.LeakyReLU(0.1)\n",
    "        self.maxpool = nn.MaxPool2d(2, 2)\n",
    "        self.conv0 = nn.Conv2d(input_size, 64, 3, stride=1, padding=0)\n",
    "        self.conv1 = nn.Conv2d(64, 64, 3, stride=1, padding=0)\n",
    "        self.conv2 = nn.Conv2d(64, 64, 3, stride=1, padding=0)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, stride=1, padding=0)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.adaptivepool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.linear1 = nn.Linear(128, 10)\n",
    "        self.dropout = nn.Dropout2d(0.5)\n",
    "        self.linear2 = nn.Linear(10, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv0(x)\n",
    "        out = self.activation(out)\n",
    "        #print(out.shape)\n",
    "        out = self.conv1(out)\n",
    "        out = self.activation(out)\n",
    "        #print(out.shape)\n",
    "        out = self.conv2(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.maxpool(out)\n",
    "        #print(out.shape)\n",
    "        out = self.conv3(out)\n",
    "        out = self.activation(out)\n",
    "\n",
    "        #print(out.shape)\n",
    "        out = self.adaptivepool(out)\n",
    "        out = self.flatten(out)\n",
    "        #print(out.shape)\n",
    "        out = self.linear1(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.linear2(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce965f09-fdf2-4739-acbc-406e463a7e52",
   "metadata": {
    "id": "ce965f09-fdf2-4739-acbc-406e463a7e52",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def accuracy(labels, predictions, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Computes accuracy between binary labels and predictions.\n",
    "\n",
    "    Parameters:\n",
    "    - labels (torch.Tensor): Binary ground truth labels (0 or 1).\n",
    "    - predictions (torch.Tensor): Predicted values (e.g., output of a sigmoid activation).\n",
    "    - threshold (float): Threshold for converting predictions to binary values.\n",
    "\n",
    "    Returns:\n",
    "    - float: Accuracy value.\n",
    "    \"\"\"\n",
    "    binary_predictions = (torch.sigmoid(predictions) > threshold).to(torch.float32)\n",
    "    correct_predictions = (binary_predictions == labels).to(torch.float32)\n",
    "    accuracy_value = correct_predictions.mean().item()\n",
    "    return accuracy_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00b2e811-0f82-40d3-96d7-f4679f402a5c",
   "metadata": {
    "id": "00b2e811-0f82-40d3-96d7-f4679f402a5c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_path = options[\"train_path\"]\n",
    "test_path = options[\"test_path\"]\n",
    "train_ds_cats_dogs = Dataset2class(train_path)\n",
    "test_ds_cats_dogs = Dataset2class(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "efadeb76-7320-4e80-9e06-62f5d8c66d8a",
   "metadata": {
    "id": "efadeb76-7320-4e80-9e06-62f5d8c66d8a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = options[\"batch_size\"]\n",
    "train_size = int(0.85 * len(train_ds_cats_dogs))\n",
    "val_size = len(train_ds_cats_dogs) - train_size\n",
    "\n",
    "train_data, val_data = random_split(train_ds_cats_dogs, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=0, drop_last=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False,  num_workers=0, drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_ds_cats_dogs, batch_size=batch_size, num_workers=0, shuffle=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54f1b7ee-0e2d-4da8-be7c-762c5c0ca296",
   "metadata": {
    "id": "54f1b7ee-0e2d-4da8-be7c-762c5c0ca296",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "ConvNet(\n",
      "  (activation): LeakyReLU(negative_slope=0.1)\n",
      "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (adaptivepool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (linear1): Linear(in_features=128, out_features=10, bias=True)\n",
      "  (dropout): Dropout2d(p=0.5, inplace=False)\n",
      "  (linear2): Linear(in_features=10, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "input_size = options[\"network\"][\"input_size\"]\n",
    "output_size = options[\"network\"][\"output_size\"]\n",
    "model = ConvNet(input_size, output_size)\n",
    "model = model.to(device)\n",
    "print(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd44c046-6738-46e3-b891-d3951b63c170",
   "metadata": {
    "id": "bd44c046-6738-46e3-b891-d3951b63c170",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "923c25a7-dc79-4fd7-a217-a86c9492a336",
   "metadata": {
    "id": "923c25a7-dc79-4fd7-a217-a86c9492a336",
    "outputId": "194d1438-8b05-49b0-c679-332798e412a7",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150805\n"
     ]
    }
   ],
   "source": [
    "print(count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "467212ac-1720-4ba5-bc6b-872d6fda15b2",
   "metadata": {
    "id": "467212ac-1720-4ba5-bc6b-872d6fda15b2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4606a5b-3e0b-4827-b777-2c38423194a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "use_amp = options[\"network\"][\"use_amp\"]\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "torch.backends.cudnn.benchmark = options[\"network\"][\"benchmark\"] #проверяет сначала какой алгоритм вычисления тензоров\n",
    "#эффективный и потом по нему производит все вычисления. Необходим фиксированный размер фотографии\n",
    "torch.backends.cudnn.deterministic = False #работает как seed=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c76574e-9ef6-499c-a02c-0036a70e1cbe",
   "metadata": {
    "id": "2c76574e-9ef6-499c-a02c-0036a70e1cbe",
    "outputId": "12d13898-04df-4eba-b4c5-9816b92cebfc",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/664 [00:00<?, ?it/s]C:\\Users\\bratk\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:1347: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n",
      "loss_train: 0.694, accuracy_train: 0.531:   3%|▎         | 20/664 [00:11<06:14,  1.72it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 28\u001b[0m     loss_item_train \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     29\u001b[0m     loss_train \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_item_train\n\u001b[0;32m     31\u001b[0m     accuracy_current_train \u001b[38;5;241m=\u001b[39m accuracy(train_labels, pred)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "    loss_val = 0\n",
    "    loss_train = 0\n",
    "    accuracy_val = 0\n",
    "    accuracy_train = 0\n",
    "    \n",
    "    model.train()\n",
    "    for sample in (pbar := tqdm(train_loader)):\n",
    "        train_img = sample[\"img\"].to(device)\n",
    "        train_labels = sample[\"labels\"].float().to(device)  # One-hot encode the labels\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast(use_amp):\n",
    "            pred = model(train_img)\n",
    "            loss = loss_fn(pred, train_labels)\n",
    "        \n",
    "        if device == \"cuda\" and use_amp:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            loss_item_train = loss.item()\n",
    "            loss_train += loss_item_train\n",
    "\n",
    "            accuracy_current_train = accuracy(train_labels, pred)\n",
    "            accuracy_train += accuracy_current_train\n",
    "        pbar.set_description(f\"loss_train: {loss_item_train:.3f}, accuracy_train: {accuracy_current_train:.3f}\")\n",
    "        \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for sample_val in tqdm(val_loader):\n",
    "            img_val = sample_val[\"img\"].to(device)\n",
    "            label_val = sample_val[\"labels\"].float().to(device)  # One-hot encode the labels\n",
    "            \n",
    "            with autocast(use_amp):\n",
    "                pred_val = model(img_val)\n",
    "                loss_fn_val = loss_fn(pred_val, label_val)\n",
    "\n",
    "                loss_item_val = loss_fn_val.item()\n",
    "                loss_val += loss_item_val\n",
    "\n",
    "                accuracy_current_val = accuracy(label_val, pred_val)\n",
    "                accuracy_val += accuracy_current_val\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} - Train Loss: {loss_train / len(train_loader):.3f}, Train Accuracy: {accuracy_train / len(train_loader):.3f}\")\n",
    "    print(f\"Epoch {epoch + 1} - Validation Loss: {loss_val / len(val_loader):.3f}, Validation Accuracy: {accuracy_val / len(val_loader):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9a702e82-76ea-429f-97d5-5664001161ac",
   "metadata": {
    "id": "9a702e82-76ea-429f-97d5-5664001161ac",
    "outputId": "d3b9479f-9e10-49f6-b65d-282984e13d53",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5197481782663436\n",
      "0.7405753968253969\n"
     ]
    }
   ],
   "source": [
    "loss_val = 0\n",
    "accuracy_val = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for sample in test_loader:\n",
    "        img, labels = sample[\"img\"].to(device), sample[\"labels\"].float().to(device)\n",
    "\n",
    "        preds = model(img)\n",
    "        loss = loss_fn(preds, labels)\n",
    "\n",
    "        loss_item = loss.item()\n",
    "        loss_val += loss_item\n",
    "        accuracy_current = accuracy(labels, preds)\n",
    "        accuracy_val += accuracy_current\n",
    "print(loss_val/len(test_loader))\n",
    "print(accuracy_val/len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bb9bfeb2-5ae4-4db9-8aeb-dafff038a477",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save({'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss_fn,\n",
    "            \"epoch\" : epochs},\n",
    "            \"best_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7106bb17-5a53-4686-97c9-4b0265aa0431",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_model = ConvNet()\n",
    "best_model = best_model.to(\"cpu\")\n",
    "optimizer = torch.optim.Adam(best_model.parameters())\n",
    "\n",
    "checkpoint = torch.load(\"best_model\")\n",
    "best_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "loss = checkpoint['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f5218732-fca0-4dc1-9c5f-bb5b85eb1000",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_img(path):\n",
    "    img = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = img.astype(np.float32)\n",
    "    img = img / 255.0\n",
    "    img = cv2.resize(img, (256, 256), interpolation=cv2.INTER_AREA)\n",
    "    img = img.transpose((2, 0, 1))\n",
    "    t_img = torch.from_numpy(img)\n",
    "    return t_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5dea0e96-1eb7-4131-a2bd-a452ee6ccdb2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6550]])\n"
     ]
    }
   ],
   "source": [
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    print(torch.sigmoid(best_model(check_img(\"C://img//Dogs and Cats//dataset//single_prediction//banana_dog.jpg\").unsqueeze(0))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569589a3-fff3-4f69-958d-e12bc83f57a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27897e6a-55d4-4f55-9a89-26f7e09bf5d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
